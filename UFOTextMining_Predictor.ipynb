{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5be80f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "executionInfo": {
     "elapsed": 501,
     "status": "error",
     "timestamp": 1671738679396,
     "user": {
      "displayName": "Oliver Bushara",
      "userId": "03905191959440203222"
     },
     "user_tz": 300
    },
    "id": "a5be80f5",
    "outputId": "cea572e8-d800-4439-953f-94e505370f61",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lesliebushara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lesliebushara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/lesliebushara/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>date_time</th>\n",
       "      <th>shape</th>\n",
       "      <th>text</th>\n",
       "      <th>city_latitude</th>\n",
       "      <th>city_longitude</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>text_token</th>\n",
       "      <th>text_string</th>\n",
       "      <th>region</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chester</td>\n",
       "      <td>VA</td>\n",
       "      <td>2019-12-12 18:43:00</td>\n",
       "      <td>light</td>\n",
       "      <td>my wife was driving southeast on a fairly popu...</td>\n",
       "      <td>37.343152</td>\n",
       "      <td>-77.408582</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[wife, driving, southeast, fairly, populated, ...</td>\n",
       "      <td>wife driving southeast fairly populated main s...</td>\n",
       "      <td>South</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rocky Hill</td>\n",
       "      <td>CT</td>\n",
       "      <td>2019-03-22 18:30:00</td>\n",
       "      <td>circle</td>\n",
       "      <td>i think that i may caught a ufo on the nbc nig...</td>\n",
       "      <td>41.664800</td>\n",
       "      <td>-72.639300</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[think, may, caught, ufo, nbc, nightly, news, ...</td>\n",
       "      <td>think may caught ufo nbc nightly news aired ma...</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ottawa</td>\n",
       "      <td>ON</td>\n",
       "      <td>2019-04-17 02:00:00</td>\n",
       "      <td>teardrop</td>\n",
       "      <td>i was driving towards the intersection of fall...</td>\n",
       "      <td>45.381383</td>\n",
       "      <td>-75.708501</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[driving, towards, intersection, fallowfield, ...</td>\n",
       "      <td>driving towards intersection fallowfield eagle...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.7506</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>NY</td>\n",
       "      <td>2009-03-15 18:00:00</td>\n",
       "      <td>cigar</td>\n",
       "      <td>in peoria, arizona, i saw a cigar shaped craft...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>[peoria, arizona, saw, cigar, shaped, craft, f...</td>\n",
       "      <td>peoria arizona saw cigar shaped craft floating...</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.9626</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kirbyville</td>\n",
       "      <td>TX</td>\n",
       "      <td>2019-04-02 20:25:00</td>\n",
       "      <td>disk</td>\n",
       "      <td>the object has flashing lights that are green,...</td>\n",
       "      <td>30.677200</td>\n",
       "      <td>-94.005200</td>\n",
       "      <td>900.0</td>\n",
       "      <td>[object, flashing, lights, green, blue, red, w...</td>\n",
       "      <td>object flashing lights green blue red white li...</td>\n",
       "      <td>South</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         city state            date_time     shape  \\\n",
       "0     Chester    VA  2019-12-12 18:43:00     light   \n",
       "1  Rocky Hill    CT  2019-03-22 18:30:00    circle   \n",
       "2      Ottawa    ON  2019-04-17 02:00:00  teardrop   \n",
       "3      Peoria    NY  2009-03-15 18:00:00     cigar   \n",
       "4  Kirbyville    TX  2019-04-02 20:25:00      disk   \n",
       "\n",
       "                                                text  city_latitude  \\\n",
       "0  my wife was driving southeast on a fairly popu...      37.343152   \n",
       "1  i think that i may caught a ufo on the nbc nig...      41.664800   \n",
       "2  i was driving towards the intersection of fall...      45.381383   \n",
       "3  in peoria, arizona, i saw a cigar shaped craft...            NaN   \n",
       "4  the object has flashing lights that are green,...      30.677200   \n",
       "\n",
       "   city_longitude  duration_sec  \\\n",
       "0      -77.408582           5.0   \n",
       "1      -72.639300           4.0   \n",
       "2      -75.708501          10.0   \n",
       "3             NaN         120.0   \n",
       "4      -94.005200         900.0   \n",
       "\n",
       "                                          text_token  \\\n",
       "0  [wife, driving, southeast, fairly, populated, ...   \n",
       "1  [think, may, caught, ufo, nbc, nightly, news, ...   \n",
       "2  [driving, towards, intersection, fallowfield, ...   \n",
       "3  [peoria, arizona, saw, cigar, shaped, craft, f...   \n",
       "4  [object, flashing, lights, green, blue, red, w...   \n",
       "\n",
       "                                         text_string     region    neg    neu  \\\n",
       "0  wife driving southeast fairly populated main s...      South  0.085  0.859   \n",
       "1  think may caught ufo nbc nightly news aired ma...  Northeast  0.000  1.000   \n",
       "2  driving towards intersection fallowfield eagle...       None  0.356  0.644   \n",
       "3  peoria arizona saw cigar shaped craft floating...  Northeast  0.012  0.791   \n",
       "4  object flashing lights green blue red white li...      South  0.097  0.776   \n",
       "\n",
       "     pos  compound sentiment  \n",
       "0  0.056    0.0516  positive  \n",
       "1  0.000    0.0000   neutral  \n",
       "2  0.000   -0.7506  negative  \n",
       "3  0.197    0.9626  positive  \n",
       "4  0.126    0.5423  positive  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: -0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#read raw\n",
    "ufo = pd.read_csv('editufo.csv')\n",
    "\n",
    "#display(ufo)\n",
    "\n",
    "#ufo_text = ufo[['text']]\n",
    "\n",
    "#display(ufo_text)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "my_stopwords = ['like', 'seemed', 'could']\n",
    "stopwords.extend(my_stopwords)\n",
    "\n",
    "# matches Unicode word characters with one or more occurrences\n",
    "regexp = RegexpTokenizer('\\w+') \n",
    "\n",
    "ufo['text'] = ufo['text'].astype(str).str.lower()\n",
    "#ufo.head(3)\n",
    "\n",
    "ufo['text_token']=ufo['text'].apply(regexp.tokenize)\n",
    "#ufo.head(3)\n",
    "\n",
    "ufo['text_token'] = ufo['text_token'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "#display(ufo)\n",
    "\n",
    "ufo['text_string'] = ufo['text_token'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))\n",
    "#ufo.head(3)\n",
    "\n",
    "all_words = ' '.join([word for word in ufo['text_string']])\n",
    "\n",
    "tokenized_words = nltk.tokenize.word_tokenize(all_words)\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(tokenized_words)\n",
    "\n",
    "##regional lists\n",
    "def classify_region(state):\n",
    "    # Define the regions\n",
    "    northeast = ['CT', 'ME', 'MA', 'NH', 'RI', 'VT', 'NJ', 'NY', 'PA']\n",
    "    midwest = ['IL', 'IN', 'MI', 'OH', 'WI', 'IA', 'KS', 'MN', 'MO', 'NE', 'ND', 'SD']\n",
    "    south = ['DE', 'FL', 'GA', 'MD', 'NC', 'SC', 'VA', 'DC', 'WV', 'AL', 'KY', 'MS', 'TN', 'AR', 'LA', 'OK', 'TX']\n",
    "    west = ['AZ', 'CO', 'ID', 'MT', 'NV', 'NM', 'UT', 'WY', 'AK', 'CA', 'HI', 'OR', 'WA']\n",
    "    \n",
    "    # Check which region the state belongs to\n",
    "    if state in northeast:\n",
    "        return 'Northeast'\n",
    "    elif state in midwest:\n",
    "        return 'Midwest'\n",
    "    elif state in south:\n",
    "        return 'South'\n",
    "    elif state in west:\n",
    "        return 'West'\n",
    "\n",
    "# Add a new column to the data frame with the region classification\n",
    "ufo['region'] = ufo['state'].apply(classify_region)\n",
    "\n",
    "# Create separate data frames for each region\n",
    "Northeast = ufo[ufo['region'] == 'Northeast']\n",
    "Midwest = ufo[ufo['region'] == 'Midwest']\n",
    "South = ufo[ufo['region'] == 'South']\n",
    "West = ufo[ufo['region'] == 'West']\n",
    "\n",
    "# Export each data frame to a CSV file\n",
    "#Northeast.to_csv('Northeast.csv', index=False)\n",
    "#Midwest.to_csv('Midwest.csv', index=False)\n",
    "#South.to_csv('South.csv', index=False)\n",
    "#West.to_csv('West.csv', index=False)\n",
    "\n",
    "top50 = fdist.most_common(50)\n",
    "fdist = pd.Series(dict(top50))\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "#sns.barplot(y=fdist.index, x=fdist.values, color='blue');\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(y=fdist.index, x=fdist.values)\n",
    "\n",
    "# sort values \n",
    "fig.update_layout(barmode='stack', yaxis={'categoryorder':'total ascending'})\n",
    "\n",
    "#show plot\n",
    "#fig.show()\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "ufo_pol = copy(ufo)\n",
    "\n",
    "ufo_pol['polarity'] = ufo_pol['text_string'].apply(lambda x: analyzer.polarity_scores(x))\n",
    "#print(ufo_pol.tail(3))\n",
    "\n",
    "# Change data structure\n",
    "ufo_pol = pd.concat(\n",
    "    [ufo_pol.drop(['polarity'], axis=1), \n",
    "     ufo_pol['polarity'].apply(pd.Series)], axis=1)\n",
    "\n",
    "\n",
    "ufo_pol['sentiment'] = ufo_pol['compound'].apply(lambda x: 'positive' if x >0 else 'neutral' if x==0 else 'negative')\n",
    "display(ufo_pol.head())\n",
    "\n",
    "# Create new variable with sentiment \"neutral,\" \"positive\" and \"negative\"\n",
    "#ufo_pol['strong_sentiment'] = ufo_pol['compound'].apply(lambda x: 'positive' if x > .5 else 'negative' if x < -.5 else 'neutral')\n",
    "#ufo_pol.head(50)\n",
    "\n",
    "#sns.countplot(y='sentiment', data=ufo_pol, palette=['#b2d8d8',\"#008080\", '#db3d13'])\n",
    "#sns.countplot(y='strong_sentiment', data=ufo_pol, palette=['#b2d8d8',\"#008080\", '#db3d13']);\n",
    "\n",
    "# Lineplot\n",
    "#g = sns.lineplot(x='date_time', y='compound', data=ufo_pol)\n",
    "\n",
    "#g.set(xticklabels=[]) \n",
    "#g.set(title='Sentiment of Description')\n",
    "#g.set(xlabel=\"2004 - 2019\")\n",
    "#g.set(ylabel=\"Sentiment\")\n",
    "#g.tick_params(bottom=False)\n",
    "\n",
    "#g.axhline(0, ls='--', c = 'grey');\n",
    "\n",
    "#ufo_pol.loc[ufo_pol['compound'].idxmax()].values\n",
    "#ufo_pol.loc[ufo_pol['compound'].idxmin()].values \n",
    "\n",
    "# Check if there are any NaN values in the 'duration_sec' and 'region' columns\n",
    "null_duration = ufo_pol['duration_sec'].isnull().any()\n",
    "null_region = ufo_pol['region'].isnull().any()\n",
    "\n",
    "# If there are any NaN values in either column, drop the rows with NaN values\n",
    "if null_duration or null_region:\n",
    "    ufo_pol.dropna(subset=['duration_sec', 'region'], inplace=True)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert the 'compound' column to a float\n",
    "#ufo_pol['compound'] = ufo_pol['compound'].astype(float)\n",
    "\n",
    "# Select the features and target column\n",
    "#X = ufo_pol[['region', 'duration_sec']]\n",
    "#y = ufo_pol['compound']\n",
    "\n",
    "# One-hot encode the categorical feature 'region'\n",
    "#X = pd.get_dummies(X, columns=['region'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create and train the model\n",
    "#model = RandomForestRegressor()\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "#score = model.score(X_test, y_test)\n",
    "#print(f'Test score: {score:.2f}')\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# Convert the 'compound' column to a float\n",
    "ufo_pol['compound'] = ufo_pol['compound'].astype(float)\n",
    "\n",
    "# Select the features and target column\n",
    "X = ufo_pol[['compound', 'duration_sec']]\n",
    "y = ufo_pol['region']\n",
    "\n",
    "# One-hot encode the categorical feature 'region'\n",
    "y = pd.get_dummies(y, columns=['region'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "score = model.score(X_test, y_test)\n",
    "print(f'Test score: {score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b2868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
